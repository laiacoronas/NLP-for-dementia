{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CODIFY THE TEXT USING BERT\n",
    "\n",
    "Python 3.8\n",
    "\n",
    "De forma local optimiza el uso del GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Shape: (1, 171, 768)\n"
     ]
    }
   ],
   "source": [
    "text = \"the scene is in the in the kitchen . the mother is wiping dishes and the water is running on the floor . a child is trying to get a boy is trying to get cookies outta a jar and hes about to tip over on a stool . uh the little girl is reacting to his falling . uh it seems to be summer out . the window is open . the curtains are blowing . it must be a gentle breeze . theres grass outside in the garden . uh mothers finished certain of the the dishes . kitchens very tidy . the mother seems to have nothing in the house to eat except cookies in the cookie jar . uh the children look to be almost about the same size . perhaps theyre twins . theyre dressed for summer warm weather . um you want more the mothers in a short sleeve dress . Ill hafta say its warm .\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertConfig, TFBertModel, BertTokenizer\n",
    "\n",
    "custom_config = BertConfig(\n",
    "    vocab_size=30522,               \n",
    "    num_attention_heads=12,         \n",
    "    num_hidden_layers=12,          \n",
    "    attention_probs_dropout_prob=0.1,  \n",
    "    hidden_size=768,                \n",
    "    intermediate_size=3072,        \n",
    "    hidden_dropout_prob=0.1,        \n",
    "    hidden_act=\"relu\",              \n",
    "    max_position_embeddings=512    \n",
    ")\n",
    "\n",
    "model = TFBertModel(custom_config)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"tf\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "bert_outputs = model(**inputs)\n",
    "\n",
    "bert_last_hidden_state = bert_outputs.last_hidden_state \n",
    "bert_pooled_output = bert_outputs.pooler_output        \n",
    "\n",
    "print(\"Last Hidden State Shape:\", bert_last_hidden_state.shape)\n",
    "\n",
    "# shape: number of samples is 1, number of tokens is 171 and size of the embedding vector is 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. CNN TEXT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vector Shape: (1, 260)\n"
     ]
    }
   ],
   "source": [
    "class TextCNN(tf.keras.Model):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # convolution kernels\n",
    "        self.conv1 = tf.keras.layers.Conv2D(130, (5, 768), activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(130, (10, 768), activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(130, (15, 768), activation='relu')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(130, (20, 768), activation='relu')\n",
    "\n",
    "        # max pooling\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling2D()\n",
    "\n",
    "        # fusion layer\n",
    "        self.fc = tf.keras.layers.Dense(260, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_prob)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # input the output of the bert model\n",
    "        x = tf.expand_dims(inputs, -1) \n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x4 = self.conv4(x)\n",
    "        \n",
    "        # pool the outputs of the convolution layers \n",
    "        pooled_1 = self.pool(x1)\n",
    "        pooled_2 = self.pool(x2)\n",
    "        pooled_3 = self.pool(x3)\n",
    "        pooled_4 = self.pool(x4)\n",
    "        \n",
    "        # fusion of all the features\n",
    "        fused_features = tf.concat([pooled_1, pooled_2, pooled_3, pooled_4], axis=-1)\n",
    "\n",
    "        feature_vector = self.fc(fused_features)\n",
    "        feature_vector = self.dropout(feature_vector)\n",
    "        \n",
    "        return feature_vector\n",
    "\n",
    "textcnn_model = TextCNN()\n",
    "feature_vector_cnn = textcnn_model(bert_last_hidden_state)  \n",
    "print(\"Feature Vector Shape:\", feature_vector_cnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. LTSM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del vector de características (LSTM): (1, 260)\n"
     ]
    }
   ],
   "source": [
    "lstm_units = 260 \n",
    "dropout_rate = 0.5\n",
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=bert_last_hidden_state.shape[1:]), \n",
    "    tf.keras.layers.LSTM(\n",
    "        units=lstm_units,\n",
    "        activation='relu',          \n",
    "        return_sequences=False      \n",
    "    ),\n",
    "    tf.keras.layers.Dropout(dropout_rate)  \n",
    "])\n",
    "\n",
    "lstm_features = lstm_model(bert_last_hidden_state)\n",
    "print(\"Shape del vector de características (LSTM):\", lstm_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to concatenate the CNN + LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Feature Vector Shape: (1, 520)\n"
     ]
    }
   ],
   "source": [
    "concatenated_vector = tf.concat([lstm_features, feature_vector_cnn], axis=-1)\n",
    "print(\"Concatenated Feature Vector Shape:\", concatenated_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fused features need to be passed into the fully connected layer first, and the Softmax classifier is used for the classification task. The dimension of the output vector must be the same as the number of categories (2 in this study) in the classification. Finally, we used the feature vector F to do the classification; y =soft max(WcF +bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction class: [[0.9726935  0.02730652]]\n",
      "Shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 520  \n",
    "num_classes = 2  \n",
    "\n",
    "classification_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),  \n",
    "    tf.keras.layers.Dense(\n",
    "        units=num_classes,  \n",
    "        activation='softmax' \n",
    "    )\n",
    "])\n",
    "\n",
    "#random example to see if the dimensions work\n",
    "y_pred = classification_model(concatenated_vector)\n",
    "\n",
    "print(\"Prediction class:\", y_pred.numpy()) #probability of belonging to each class\n",
    "print(\"Shape:\", y_pred.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "Example: https://www.tensorflow.org/tutorials/keras/classification?\n",
    "\n",
    "We have not trained the model, so the last prediction is randomly made.\n",
    "\n",
    "For the training of the network, the paper mentions the following:\n",
    "\n",
    "- epoch: 10\n",
    "- batch size: 16\n",
    "- learning rate: 1e-5\n",
    "- dropout: 0.2\n",
    "- max grad norm: 10\n",
    "- train: test is proportion 7:3\n",
    "- 10 consecutive runs\n",
    "- performance: accuracy\n",
    "- loss: crossentropy\n",
    "- optimizer: AdamW, where W stands for weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden States Tensor Shape: (101, 1, 512, 768)\n",
      "Pooler Outputs Tensor Shape: (101, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "custom_config = BertConfig(\n",
    "    vocab_size=30522,               \n",
    "    num_attention_heads=12,        \n",
    "    num_hidden_layers=12,          \n",
    "    attention_probs_dropout_prob=0.1,  \n",
    "    hidden_size=768,               \n",
    "    intermediate_size=3072,        \n",
    "    hidden_dropout_prob=0.1,       \n",
    "    hidden_act=\"relu\",             \n",
    "    max_position_embeddings=512    \n",
    ")\n",
    "bert_model = TFBertModel(custom_config)\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\lclai\\\\Desktop\\\\transcripts_cleaned.csv\")\n",
    "data = data[[\"label\", \"clean_transcripts\"]]\n",
    "\n",
    "def preprocess_and_get_bert_embeddings(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"tf\",\n",
    "        padding='max_length',  \n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    bert_outputs = model(**inputs)\n",
    "    return bert_outputs.last_hidden_state, bert_outputs.pooler_output  # Obtener la última capa oculta y la salida de pooling\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "last_hidden_states = []\n",
    "pooler_outputs = []\n",
    "\n",
    "for text in data['clean_transcripts']:\n",
    "    last_hidden_state, pooler_output = preprocess_and_get_bert_embeddings(text, tokenizer, bert_model)\n",
    "    last_hidden_states.append(last_hidden_state.numpy())  \n",
    "    pooler_outputs.append(pooler_output.numpy())\n",
    "\n",
    "last_hidden_states_tensor = tf.convert_to_tensor(last_hidden_states, dtype=tf.float32)\n",
    "pooler_outputs_tensor = tf.convert_to_tensor(pooler_outputs, dtype=tf.float32)\n",
    "\n",
    "print(f\"Last Hidden States Tensor Shape: {last_hidden_states_tensor.shape}\")\n",
    "print(f\"Pooler Outputs Tensor Shape: {pooler_outputs_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (70, 1, 512, 768), x test shape: (31, 1, 512, 768)\n",
      "y train shape: (70,), y test shape: (31,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = last_hidden_states_tensor.numpy()\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"x train shape: {X_train.shape}, x test shape: {X_test.shape}\")\n",
    "print(f\"y train shape: {y_train.shape}, y test shape: {y_test.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
